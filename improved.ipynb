{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from pyspark.ml.clustering import GaussianMixture\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BGMM_Local\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .getOrCreate()\n",
    "from pyspark.sql.functions import explode, array\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.sql.functions import col\n",
    "import pyarrow as pa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## imporved transformer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformer():\n",
    "    \n",
    "    \"\"\"\n",
    "    Transformer class responsible for processing data to train the CTABGANSynthesizer model\n",
    "    \n",
    "    Variables:\n",
    "    1) train_data -> input dataframe \n",
    "    2) categorical_list -> list of categorical columns\n",
    "    3) mixed_dict -> dictionary of mixed columns\n",
    "    4) n_clusters -> number of modes to fit bayesian gaussian mixture (bgm) model\n",
    "    5) eps -> threshold for ignoring less prominent modes in the mixture model \n",
    "    6) ordering -> stores original ordering for modes of numeric columns\n",
    "    7) output_info -> stores dimension and output activations of columns (i.e., tanh for numeric, softmax for categorical)\n",
    "    8) output_dim -> stores the final column width of the transformed data\n",
    "    9) components -> stores the valid modes used by numeric columns\n",
    "    10) filter_arr -> stores valid indices of continuous component in mixed columns\n",
    "    11) meta -> stores column information corresponding to different data types i.e., categorical/mixed/numerical\n",
    "\n",
    "\n",
    "    Methods:\n",
    "    1) __init__() -> initializes transformer object and computes meta information of columns\n",
    "    2) get_metadata() -> builds an inventory of individual columns and stores their relevant properties\n",
    "    3) fit() -> fits the required bgm models to process the input data\n",
    "    4) transform() -> executes the transformation required to train the model\n",
    "    5) inverse_transform() -> executes the reverse transformation on data generated from the model\n",
    "    \n",
    "    \"\"\"   \n",
    "    def __init__(self, train_data=pl.DataFrame, categorical_list=[], mixed_dict={}, n_clusters=10, eps=0.005):\n",
    "        \n",
    "        self.meta = None\n",
    "        self.train_data = train_data\n",
    "        self.categorical_columns= categorical_list\n",
    "        self.mixed_columns= mixed_dict\n",
    "        self.n_clusters = n_clusters\n",
    "        self.eps = eps\n",
    "        self.ordering = []\n",
    "        self.output_info = []\n",
    "        self.output_dim = 0\n",
    "        self.components = []\n",
    "        self.filter_arr = []\n",
    "        self.meta = self.get_metadata()\n",
    "\n",
    "    def get_metadata(self):\n",
    "        meta = []\n",
    "\n",
    "        for index in self.train_data.columns:\n",
    "            column = self.train_data[index]\n",
    "            if index in self.categorical_columns:\n",
    "                mapper = column.value_counts().sort('count', descending= True).select(pl.col(index)).to_numpy().tolist()\n",
    "                meta.append({\n",
    "                        \"name\": index,\n",
    "                        \"type\": \"categorical\",\n",
    "                        \"size\": len(mapper),\n",
    "                        \"i2s\": mapper\n",
    "                })\n",
    "            elif index in self.mixed_columns.keys():\n",
    "                meta.append({\n",
    "                    \"name\": index,\n",
    "                    \"type\": \"mixed\",\n",
    "                    \"min\": column.min(),\n",
    "                    \"max\": column.max(),\n",
    "                    \"modal\": self.mixed_columns[index]\n",
    "                })\n",
    "            else:\n",
    "                meta.append({\n",
    "                    \"name\": index,\n",
    "                    \"type\": \"continuous\",\n",
    "                    \"min\": column.min(),\n",
    "                    \"max\": column.max(),\n",
    "                })       \n",
    "        return meta\n",
    "    \n",
    "    def fit(self):\n",
    "        # stores the corresponding bgm models for processing numeric data\n",
    "        model = []\n",
    "        \n",
    "        # iterating through column information\n",
    "        for id_, info in enumerate(self.meta):\n",
    "            if info['type'] == \"continuous\":\n",
    "                # fitting bgm model from SparkML to handle partitioned large datasets\n",
    "                gm = GaussianMixture(\n",
    "                    k=self.n_clusters,\n",
    "                    maxIter=100,\n",
    "                    seed = 42, \n",
    "                    )\n",
    "                current_column = info['name']\n",
    "                current_data = self.train_data.select(pl.col(current_column))\n",
    "\n",
    "                # calculate number of partitions - this affects the speed of model fitting and may need to be retuned\n",
    "                num_partitions = max(10, int(current_data.shape[0]/50000))\n",
    "\n",
    "                # saving a polars DF to parquet and reading in spark is fastest way of converting without keeping in memory\n",
    "                current_data.write_parquet('temp.parquet')\n",
    "                del current_data\n",
    "                data_temp = spark.read.parquet('temp.parquet')\n",
    "                data_temp = data_temp.repartition(num_partitions)\n",
    "                assembler = VectorAssembler(inputCols=[current_column], outputCol=\"features\")\n",
    "                transformed_df = assembler.transform(data_temp)\n",
    "\n",
    "                # fit GMM \n",
    "                print(f'fitting model for {current_column}')\n",
    "                gm_fit = gm.fit(transformed_df)\n",
    "                model.append(gm_fit)\n",
    "                \n",
    "                # means and stds of the modes are obtained from the corresponding fitted bgm model\n",
    "                m_cov = gm_fit.gaussiansDF.toPandas()\n",
    "                # Flatten 'mean' column (extract the first element from the list)\n",
    "                m_cov['mean'] = m_cov['mean'].apply(lambda x: x[0])\n",
    "\n",
    "                # Extract scalar value from DenseMatrix in the 'cov' column\n",
    "                m_cov['cov'] = m_cov['cov'].apply(lambda x: x[0, 0] if hasattr(x, 'toArray') else x)\n",
    "                m_cov['stds'] = np.sqrt(m_cov['cov'])\n",
    "                m_cov['feat_num'] = m_cov.index\n",
    "\n",
    "                m_cov = pl.DataFrame(m_cov[['mean','stds', 'feat_num']])\n",
    "\n",
    "                # Save the means and std.dev of features in meta which can be referred later\n",
    "                info['m_cov'] = m_cov\n",
    "\n",
    "                self.model = model\n",
    "                print(f'model fit completed for {current_column}')\n",
    "                # keeping only relevant modes that have higher weight than eps and are used to fit the data\n",
    "                old_comp = np.array(gm_fit.weights) > self.eps\n",
    "\n",
    "                #mode_freq = tuple(current_data[current_column].value_counts().sort('count', descending= True).select(pl.col(current_column)).to_numpy())\n",
    "                mode_freq = list(range(self.n_clusters))\n",
    "                comp = []\n",
    "                for i in range(self.n_clusters):\n",
    "                    if (i in (mode_freq)) & old_comp[i]:\n",
    "                        comp.append(True)\n",
    "                    else:\n",
    "                        comp.append(False)\n",
    "                self.components.append(comp) \n",
    "                self.output_info += [(1, 'tanh'), (np.sum(comp), 'softmax')]\n",
    "                self.output_dim += 1 + np.sum(comp)\n",
    "        \n",
    "        self.model = model\n",
    "\n",
    "\n",
    "    def transform(self, data: pl.DataFrame, save_parquet:bool = True):\n",
    "\n",
    "\n",
    "        # stores the transformed values\n",
    "        values = []\n",
    "\n",
    "        # used for accessing filter_arr for transforming mixed columns\n",
    "        mixed_counter = 0\n",
    "\n",
    "        os.makedirs('feature_values/', exist_ok=True)\n",
    "\n",
    "        # iterating through column information\n",
    "        for id_, info in enumerate(self.meta):\n",
    "            if info['type'] == \"continuous\":\n",
    "\n",
    "                current_column = info['name']\n",
    "                current_data = data.select(pl.col(current_column))\n",
    "                current_data = current_data.with_row_index()\n",
    "                \n",
    "                # get saved means and std.devs for features\n",
    "                m_cov = info['m_cov']\n",
    "                \n",
    "\n",
    "                features = current_data.join(m_cov, how = 'cross')\n",
    "                features = features.with_columns(((pl.col(current_column) - pl.col('mean'))/(4* pl.col('stds'))).alias('feat_value'))\n",
    "\n",
    "                # number of distict modes\n",
    "                n_opts = sum(self.components[id_])          \n",
    "                # storing the mode for each data point by sampling from the probability mass distribution across all modes based on fitted bgm model \n",
    "                # create sparkml vectors for predicting class probabilities\n",
    "                num_partitions = max(10, int(current_data.shape[0]/50000))\n",
    "                current_data.write_parquet('temp.parquet')\n",
    "                del current_data\n",
    "                data_temp = spark.read.parquet('temp.parquet')\n",
    "                data_temp = data_temp.repartition(num_partitions)\n",
    "                assembler = VectorAssembler(inputCols=[current_column], outputCol=\"features\")\n",
    "                transformed_df = assembler.transform(data_temp)\n",
    "\n",
    "                # create polars df with predicted probaility\n",
    "                probs = self.model[id_].transform(transformed_df).select('probability')\n",
    "                probs = probs.select(vector_to_array(\"probability\").alias(\"probability\"))\n",
    "                probs = probs.select(*[col('probability').getItem(i).alias(f'prob_{i}') for i in range(0, n_opts+1)])\n",
    "\n",
    "                col_names = probs.columns\n",
    "                columns_to_select = [col for col, to_select in zip(col_names, self.components[id_]) if to_select]\n",
    "                probs = probs.select(columns_to_select)\n",
    "                probs = pl.from_arrow(pa.Table.from_batches(probs._collect_as_arrow()))\n",
    "\n",
    "                #relevel probability\n",
    "                probs = probs.with_columns(sum = pl.sum_horizontal(pl.all()))\n",
    "                probs = probs.with_columns(pl.all()/pl.col('sum'))\n",
    "                probs = probs.drop('sum')\n",
    "                \n",
    "                # select most probable feature \n",
    "                feature_options = [i for i in range(n_opts)]\n",
    "                probs = probs.with_columns(pl.concat_list(pl.all()).alias('prob_list'))\n",
    "                probs = probs.with_columns(pl.col(\"prob_list\").map_elements(lambda prob_list: np.random.choice(feature_options, p = prob_list) , return_dtype=pl.Int64).alias(\"sel_feature\"))\n",
    "                probs = probs.with_row_index()\n",
    "\n",
    "                # obtaining the normalized values based on the appropriately selected mode and clipping to ensure values are within (-1,1)\n",
    "                #select only appropriate features\n",
    "                sel_feat_num = list(range(self.n_clusters))\n",
    "                sel_feat_num = [col for col, to_select in zip(sel_feat_num, self.components[id_]) if to_select]\n",
    "                \n",
    "\n",
    "                # select feature value for selected random component \n",
    "                features = features[['index', 'feat_value', 'feat_num']].join(probs[['index', 'sel_feature']], how = 'inner', left_on=['index', 'feat_num'], right_on= ['index', 'sel_feature'])\n",
    "                # clip feat value\n",
    "                features = features.with_columns(pl.col('feat_value').clip(-0.99, 0.99))\n",
    "\n",
    "                feature_rank = probs['sel_feature'].value_counts().sort(pl.col('count'),descending=True).with_row_index('rank')\n",
    "                feature_rank = feature_rank[['rank', 'sel_feature']]\n",
    "                \n",
    "                features = features.join(feature_rank, how = 'left', coalesce= True, left_on= ['feat_num'], right_on= ['sel_feature'])\n",
    "\n",
    "                # values += (current_column, features)\n",
    "                # Save feature values - feature values are stored as a long DataFrame instead of wide table with feature value and one hot encoded selected feature number\n",
    "                # the DF has information of feature number, calculated feature value \n",
    "                \n",
    "                features.write_parquet(f'feature_values/{current_column}.parquet')\n",
    "                print(f'saved intermediate feature value for column : {current_column}')\n",
    "\n",
    "                # storing the original ordering for invoking inverse transform\n",
    "                self.ordering.append(feature_rank)\n",
    "                info['feature_rank'] = feature_rank\n",
    "\n",
    "            values = pl.DataFrame()\n",
    "\n",
    "            # iterating through column information\n",
    "\n",
    "        #following block converts the long format of saved feature values to desired format. \n",
    "        # the output is still a parquet or polars DF (will need changes downline)\n",
    "        for id_, info in enumerate(self.meta):\n",
    "            if info['type'] == \"continuous\":\n",
    "                current_column = info['name']\n",
    "                feature_rank = info['feature_rank']\n",
    "                n_features = feature_rank.shape[0]\n",
    "                feature_val = pl.read_parquet(f'feature_values/{current_column}.parquet')\n",
    "                feature_val = feature_val.with_columns([\n",
    "                                (pl.col(\"rank\") == i).cast(pl.Int64).alias(f\"{current_column}_{i}\")\n",
    "                                for i in range(n_features) ])\n",
    "                sel_columns = ['feat_value'] + [f\"{current_column}_{i}\" for i in range(n_features) ]\n",
    "                feature_val = feature_val[sel_columns]\n",
    "                feature_val = feature_val.rename({'feat_value':current_column})\n",
    "                values = pl.concat([values, feature_val], how = 'horizontal')\n",
    "\n",
    "        # Save transformed values\n",
    "        if save_parquet:\n",
    "            os.makedirs('transformed_values/', exist_ok= True)\n",
    "            parquet_path = f'transformed_values/transformed_values.parquet'\n",
    "            values.write_parquet(parquet_path)\n",
    "            print(f'parquets saved at {parquet_path} ')\n",
    "        else:\n",
    "            return values\n",
    "\n",
    "    def inverse_transform(self, data: pl.DataFrame, save_parquet:bool = True):\n",
    "\n",
    "        inverted_data = pl.DataFrame()\n",
    "        # iterating through column information\n",
    "\n",
    "        for id_, info in enumerate(self.meta):\n",
    "            if info['type'] == \"continuous\":\n",
    "                current_column = info['name']\n",
    "                feature_rank = info['feature_rank']\n",
    "                m_cov = info['m_cov']\n",
    "                n_features = feature_rank.shape[0]\n",
    "\n",
    "                # first reverse the one hot encoded columns to get feature rank\n",
    "                ohe_cols =  [f\"{current_column}_{i}\" for i in range(n_features) ]\n",
    "                ohe_df =  data[ohe_cols]\n",
    "                ohe_df = ohe_df.with_columns(pl.concat_list(pl.all()).alias(\"rank\"))\n",
    "                ohe_df = ohe_df.with_columns(pl.col('rank').list.arg_max())\n",
    "\n",
    "                # get feature number from rank and get std and mean\n",
    "\n",
    "                inreversing_df = ohe_df[['rank']].join(feature_rank, on = ['rank'], how = 'left', coalesce= True)\n",
    "                inreversing_df = inreversing_df.join(m_cov, how = 'left', left_on = ['sel_feature'], right_on= ['feat_num'], coalesce= True)\n",
    "\n",
    "                # join feature value and inversing df \n",
    "                inverted_df = pl.concat([data[[current_column]], inreversing_df], how = 'horizontal')\n",
    "                inverted_df = inverted_df.with_columns((pl.col(current_column) * (4* pl.col('stds')) + pl.col('mean')).alias(current_column))\n",
    "                inverted_df = inverted_df[[current_column]]\n",
    "                inverted_data = pl.concat([inverted_data, inverted_df], how = 'horizontal')\n",
    "            if save_parquet:\n",
    "                os.makedirs('inverted_values/', exist_ok= True)\n",
    "                inverted_data.write_parquet(f'inverted_values/inverted_values.parquet')\n",
    "                print('parquet saved')\n",
    "            else: \n",
    "                return inverted_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize and scale data for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Size - 99_684, 2\n"
     ]
    }
   ],
   "source": [
    "# Initialize data \n",
    "train_data = pl.read_csv(\"Credit.csv\")[[\"Amount\", \"V1\"]]\n",
    "\n",
    "# for increasing scale. change number to scale by double. i.e. 3 will make the data 2^3 long.\n",
    "for _ in range(1):\n",
    "    train_data = pl.concat([train_data,train_data])\n",
    "\n",
    "print(f\"Data Size - {train_data.shape[0]:_}, {train_data.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## example - saving parquets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting model for Amount\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/20 18:55:30 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/11/20 18:55:31 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: CodeCache is full. Compiler has been disabled.\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: Try increasing the code cache size using -XX:ReservedCodeCacheSize=\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeCache: size=131072Kb used=23182Kb max_used=23749Kb free=107889Kb\n",
      " bounds [0x000000010a1f8000, 0x000000010b948000, 0x00000001121f8000]\n",
      " total_blobs=10474 nmethods=8630 adapters=1756\n",
      " compilation: disabled (not enough contiguous free space left)\n",
      "model fit completed for Amount\n",
      "fitting model for V1\n",
      "model fit completed for V1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved intermediate feature value for column : Amount\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved intermediate feature value for column : V1\n",
      "parquets saved at transformed_values/transformed_values.parquet \n",
      "parquet saved\n",
      "parquet saved\n",
      "CPU times: user 1.9 s, sys: 261 ms, total: 2.16 s\n",
      "Wall time: 17.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create transformer \n",
    "# transformer only addresses continous columns right now.\n",
    "transformer = DataTransformer(train_data=train_data)\n",
    "# fit models using SparkML\n",
    "transformer.fit()\n",
    "# transform other data \n",
    "transformer.transform(train_data, save_parquet= True)\n",
    "# inverse transform data \n",
    "transformer.inverse_transform(pl.read_parquet('transformed_values/transformed_values.parquet'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## example - not saving parquets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting model for Amount\n",
      "model fit completed for Amount\n",
      "fitting model for V1\n",
      "model fit completed for V1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved intermediate feature value for column : Amount\n",
      "saved intermediate feature value for column : V1\n",
      "CPU times: user 1.94 s, sys: 427 ms, total: 2.36 s\n",
      "Wall time: 11.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create transformer \n",
    "# transformer only addresses continous columns right now.\n",
    "transformer = DataTransformer(train_data=train_data)\n",
    "# fit models using SparkML\n",
    "transformer.fit()\n",
    "# transform other data \n",
    "transformed_df = transformer.transform(train_data, save_parquet= False)\n",
    "# inverse transform data \n",
    "inverse_transformed_df = transformer.inverse_transform(transformed_df, save_parquet= False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
